{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hacettepe University Computer Engineering Department\n",
    "#### BBM-409 Machine Learning Lab - Assignment 1\n",
    "#### Book-Recommendation\n",
    "##### Name/Surname: Furkan Çağlar Gülmez\n",
    "##### Id: 21527027 \n",
    "##### Advisors: Dr. Aykut Erdem, T.A. Necva Bölücü\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbor Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a-)** Show that if in both 1-NN(S1) and 1-NN(S2) the label of point x is positive, then in 1-NN(S1 ∪ S2) the label of x is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     -> While doing 1-NN(S1∪S2) classification x point is positive for both classifications,\n",
    "     the nearest neighbour is positive so that union of s1 and s2 will be the same as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b-)** Show an example such that in both 3-NN(S1) and 3-NN(S2) the label of x is positive, and in 3-NN(S1 ∪ S2) the label of x is negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -> While doing 3NN(S1∪S2) classification x point would be negative, \n",
    "    if the 2 points of the 3NN were negative after the classification of the union."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.**\n",
    "* What value of k minimizes training set error for this data set, and what is the resulting training set error? Why is training set error not a reasonable estimate of test set error, especially given this value of k?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -> A point can be its own neighbour. So, k = 0 minimizes the training set error. The error is 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What value of k minimizes the leave-one-out cross-validation error for this data set, and what is the resulting error? Why is cross-validation a better measure of test set performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -> k = 5 or k = 7 minimizes the leave-one-out cross-validation error. The error is 4/14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why might using too large values k be bad in this dataset? Why might too small values of k also be bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -> Too big k (ex: k=13) missclassifies every datapoint. Too small k leads to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sketch the 1-nearest neighbor decision boundary for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2_d.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Suppose you have m=23 training examples with n=5 features \n",
    "    (excluding the ad- ditional all-ones feature for the bias term, which you should add).\n",
    "    The closed form solution is θ = (XT X)−1XT y. For the given values of m and n, \n",
    "    what are the dimensions of X, y, θ in this equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  -> X is 23×6, y is 23×1, θ is 6×1\n",
    "    X has m rows and n + 1 columns (+1 because of the x0=1 term. y is an m-vector. θ is an (n+1)-vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Suppose you have m=50 training examples which are represented with n=200,000 dimensional feature vectors. \n",
    "    You want to use multivariate linear regression to fit paremeters θ to our data. \n",
    "    Should you prefer gradient descent or the closed form solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   -> Gradient descent, since (XTX)−1 will be very slow to compute in the normal equation.\n",
    "       With n = 200000 features, you have to invert a 200001 x 200001 matrix to compute the normal equation.\n",
    "       Inverting such a large matrix is computationally expensive, so **gradient descent** is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Which of the following are valid reasons for using feature scaling?\n",
    "        (a) It speeds up solving for θ using the normal equation.\n",
    "        (b) It prevents the matrix XT X (used in the normal equation) from being non- invertable (singular/degenerate).\n",
    "        (c) It speeds up gradient descent by making it require fewer iterations to get to a good solution.\n",
    "        (d) It is necessary to prevent gradient descent from getting stuck in local optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   -> Only **(d) is true** because feature scaling speeds up gradient descent by avoiding many extra iterations \n",
    "       that are required when one or more features take on much larger values than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Book Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Used Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "Pandas is highly optimized for performance, with critical code paths written in Cython or C.\n",
    "Reading txt file line by line and split each line takes much more time than pandas, When we have huge input file, so that I used pandas to read and split csv file and to create data frame.\n",
    "DataFrame is likely dict-like container. It has labeled rows and columns. And arithmetic operations is easy when using dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. The ancestor of NumPy, Numeric, was originally created by Jim Hugunin with contributions from several other developers. In 2005, Travis Oliphant created NumPy by incorporating features of the competing Numarray into Numeric, with extensive modifications. NumPy is open-source software and has many contributors.\n",
    "I used numpy when I calculate cosine similarity and pearson correlation of two vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6255432421712244\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def pearson(list1, list2):\n",
    "    return np.corrcoef(list1, list2)[0][1]\n",
    "print(pearson([3,-5,7,9], [2,4,6,8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson correlation of two vectors' result is 2x2 matrix, which is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corr( X, X )        Corr( X, Y )**\n",
    "\n",
    "**Corr( Y, X )        Corr( Y, Y )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I take [0,1] of result because [0,0] is the correlation between X and itself, which must be 1 and also [1,1] is the correlation between Y and itself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7128324356402513\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(list1, list2):\n",
    "    return np.dot(list1, list2) / ((np.linalg.norm(list1)) * (np.linalg.norm(list2)))\n",
    "print(cos_sim([3,-5,7,9], [2,4,6,8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I used pearson correlation** to calculate the similarities between users because results were more realistic than cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Sklearn library to split Train dataset into two part which are Train and Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data is 10 percent of actual dataset remaining part is train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets, Filtering, Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BX-Users:** userID, Location, Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BX-Books:** ISBN, Book Title, Book Author, Year Of Publication, Publisher, Small-Medium-Large Size of Image URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BX-Ratings:** User ID, ISBN, Book Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the data that I don't use, I dropped unnecessary columns for my purpose and merge datasets into one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BX-Users dropped columns are Location and Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BX-Books dropped columns are Author, Publication Year, Publisher, Image URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BX-Ratings I didn't drop any column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also dataset contains too much 0 rate and I also eliminate them with the code below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "merge_ratings_books_users = merge_ratings_books_users[merge_ratings_books_users['Book-Rating']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I filtered the data, I eliminate **users which voted less then 1 and books that are voted less then 1** with the code below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "frame_groupby_ISBN = merge_ratings_books_users.groupby(['ISBN']).filter(lambda x: (len(x) >= 1))\n",
    "frame_groupby_User_ID = frame_groupby_ISBN.groupby(['User-ID']).filter(lambda x: (len(x)) >= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I did this filterings 8950 users and 25800 ratings remaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson(list1, list2):\n",
    "    return np.corrcoef(list1, list2)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List1 is rating vector of user that we try to find neighbours to her/him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List2 is other users ratings vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(most_similar_users, k):\n",
    "    return sum(neighbour[2] for neighbour in most_similar_users if neighbour[2] != 0) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_knn(most_similar_users):\n",
    "    total_neighbour_rate = 0\n",
    "    total_neighbour_sim = 0\n",
    "\n",
    "    for i in range(len(most_similar_users)):\n",
    "\n",
    "        neighbour_sim = most_similar_users[i][1]\n",
    "        neighbour_rate = most_similar_users[i][2]\n",
    "\n",
    "        if neighbour_rate != 0:\n",
    "            total_neighbour_rate += neighbour_sim * neighbour_rate\n",
    "            total_neighbour_sim += neighbour_sim\n",
    "\n",
    "    return total_neighbour_rate / total_neighbour_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_similar_users is an array that holds neighbours as tuples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tuple there is, **(User-ID, Similarity, Rating for specified ISBN)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_user(dict_train, isbn):\n",
    "\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for user in dict_train:\n",
    "        user_rate = dict_train[user][isbn]\n",
    "        if user_rate != 0:\n",
    "            total += user_rate\n",
    "            count += 1\n",
    "\n",
    "    return total / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no user id that is specified in test data, Program calculates average ratings of all books in train dataset and predict it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_book(dict_train, user_id):\n",
    "\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for isbn in dict_train[user_id]:\n",
    "        rate = dict_train[user_id][isbn]\n",
    "        if rate != 0:\n",
    "            total += rate\n",
    "            count += 1\n",
    "\n",
    "    return total / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no isbn that is specified in test data, Program calculates average ratings which is belong to that user in train dataset and predict it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mae_plot.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it seen from the figure weighted knn is the good option for that problem because It predicts more accurate ratings than normal knn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best k value for my solution is 11. If k = 11, mean absolute error is 1.4465004892688917 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other values of k mean absolute error is always between 1.450 - 1.455."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mGalarnyk/datasciencecoursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/my-journey-to-building-book-recommendation-system-5ec959c41847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
